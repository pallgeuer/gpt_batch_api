# Hydra configuration parameters YAML

###############
##  General  ##
###############

# OpenAI API key (see openai.OpenAI, ends up in request headers)
openai_api_key: null
# OpenAI organization (see openai.OpenAI, ends up in request headers)
openai_organization: null
# OpenAI project (see openai.OpenAI, ends up in request headers)
openai_project: null
# Base URL to use for the OpenAI API client (see openai.OpenAI, servers other than OpenAI's servers can be configured to expose an OpenAI API with suitable endpoints)
client_base_url: null

#####################
##  GPT Requester  ##
#####################

# Whether to automatically create the GPT working directory if it does not exist (parent directory must already exist)
autocreate_working_dir: True
# Timeout (if any) to use when attempting to lock exclusive access to the files in the GPT working directory corresponding to the given name prefix (see utils.LockFile)
lock_timeout: null
# Lock file polling interval (see utils.LockFile)
lock_poll_interval: null
# Lock file status update interval (see utils.LockFile)
lock_status_interval: null
# Warning mode to use for internal token estimator (see tokens.TokenEstimator)
token_estimator_warn: once
# Interval in multiples of which to update remote batch states when waiting for remote batches to finish (seconds)
remote_update_interval: 10.0

 # Maximum number of requests allowed in a batch
max_batch_requests: 50000
# Maximum allowed batch size in MB (not MiB)
max_batch_mb: 100
# Maximum number of tokens to include in a single batch (in units of 1000)
max_batch_ktokens: 2000
# Maximum number of unpushed local batches at any one time
max_unpushed_batches: 10
# Maximum number of remote batches at any one time (0 = Only prepare local batches and don't push any yet)
max_remote_batches: 100
# Maximum number of requests across all uploaded remote batches at any one time
max_remote_requests: 5000000
# Maximum allowed total size in MB (not MiB) of all uploaded remote batches at any one time
max_remote_mb: 10000
# Maximum allowed total number of tokens (in units of 1000) across all uploaded remote batches at any one time
max_remote_ktokens: 5000
# Safety factor to use when comparing token counts to specified maximum values (token counts are ultimately approximations until the batch is actually executed, so a safety factor can be useful in ensuring that token limits are truly never exceeded in practice)
max_token_safety: 1.05
# EOF
